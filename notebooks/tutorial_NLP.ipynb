{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is TfidfVectorizer?\n",
    "The `TfidfVectorizer` is a tool provided by the `scikit-learn` library in Python for text feature extraction. It transforms text data into a matrix of TF-IDF features, which stands for Term Frequency-Inverse Document Frequency. This transformation helps in converting textual data into numerical data that machine learning models can process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is TF-IDF Calculated?\n",
    "TF-IDF stands for Term Frequency-Inverse Document Frequency. It is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Term Frequency (TF)**:\n",
    "   - This measures how frequently a term appears in a document. It is calculated as:\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Inverse Document Frequency (IDF)**:\n",
    "   - This measures how important a term is across the entire corpus. It is calculated as:\n",
    "     $$\n",
    "     \\text{IDF}(t) = \\log \\left( \\frac{\\text{Total number of documents}}{\\text{Number of documents containing term } t} \\right)\n",
    "     $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **TF-IDF**:\n",
    "   - This is the product of TF and IDF:\n",
    "     $$\n",
    "     \\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "     $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Does the CountVectorizer Differ from the TfidfVectorizer?\n",
    "- **CountVectorizer**:\n",
    "  - Converts text data into a matrix of token counts. It simply counts the number of occurrences of each term in each document.\n",
    "  - Example: If the term 'data' appears 3 times in a document, the CountVectorizer will assign it a value of 3.\n",
    "\n",
    "- **TfidfVectorizer**:\n",
    "  - Converts text data into a matrix of TF-IDF features, which take into account both the frequency of a term in a document and how common the term is across the entire corpus.\n",
    "  - Example: The term 'data' might appear frequently in many documents, so its TF-IDF value will be lower than a term that appears frequently in only a few documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Instantiate a TfidfVectorizer with Specific Parameters\n",
    "To create a `TfidfVectorizer` with the specified parameters, you can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "no_features = 1000  # Example value for the number of features\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the Methods of the TfidfVectorizer Object?\n",
    "The `TfidfVectorizer` object has several important methods, including:\n",
    "\n",
    "- **fit**: Learns the vocabulary and IDF from the training set.\n",
    "- **transform**: Transforms the documents to the TF-IDF representation using the learned vocabulary and IDF.\n",
    "- **fit_transform**: Combines the `fit` and `transform` methods to learn the vocabulary and IDF and transform the documents in one step.\n",
    "- **get_feature_names_out**: Returns the feature names (terms) in the learned vocabulary.\n",
    "- **vocabulary_**: Returns the learned vocabulary as a dictionary.\n",
    "- **idf_**: Returns the inverse document frequency values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is NMF?\n",
    "NMF stands for Non-negative Matrix Factorization. It is a group of algorithms in multivariate analysis and linear algebra where a matrix \\( V \\) is factorized into (usually) two matrices \\( W \\) and \\( H \\) such that:\n",
    "\n",
    "$$\n",
    "V \\approx WH\n",
    "$$\n",
    "\n",
    "NMF is used for dimensionality reduction, topic modeling, and source separation, with the constraint that all three matrices \\( V \\), \\( W \\), and \\( H \\) have no negative elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is LDA?\n",
    "LDA stands for Latent Dirichlet Allocation. It is a generative probabilistic model used in natural language processing and machine learning to discover abstract topics within a collection of documents. LDA assumes that documents are mixtures of topics and that topics are mixtures of words. The process involves:\n",
    "\n",
    "- Assigning topics to each word in a document.\n",
    "- Estimating the distribution of topics in each document.\n",
    "- Estimating the distribution of words in each topic.\n",
    "\n",
    "LDA is widely used for topic modeling and discovering hidden thematic structures in large corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import TfidfVectorizer and CountVectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fetch_20newsgroups from sklearn.datasets\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import NMF and LatentDirichletAllocation from sklearn\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_features = 100\n",
    "no_topics = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate TfidfVectorizer with specified parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **TfidfVectorizer**:\n",
    "   - `max_df=0.95`: Ignore terms that appear in more than 95% of the documents.\n",
    "   - `min_df=2`: Ignore terms that appear in fewer than 2 documents.\n",
    "   - `max_features=no_features`: Limit the number of features (terms) to `no_features`.\n",
    "   - `stop_words='english'`: Remove common English stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the fit_transform method to transform the documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Transform the Documents**:\n",
    "   - `fit_transform(documents)`: Learn the vocabulary and idf, then return the term-document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names from TfidfVectorizer\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Get Feature Names**:\n",
    "   - `get_feature_names_out()`: Retrieve the feature names (terms) learned by the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 100\n",
      "First 10 feature names: ['00' '10' '12' '14' '15' '16' '20' '25' 'a86' 'available']\n"
     ]
    }
   ],
   "source": [
    "# Print the number of features and the first 10 feature names\n",
    "print(f\"Number of features: {len(tfidf_feature_names)}\")\n",
    "print(f\"First 10 feature names: {tfidf_feature_names[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate NMF and fit_transform the TF-IDF data\n",
    "num_topics = 100 \n",
    "nmf_model = NMF(n_components=num_topics, random_state=1)\n",
    "nmf_topics = nmf_model.fit_transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **NMF (Non-negative Matrix Factorization)**:\n",
    "   - `NMF(n_components=num_topics, random_state=1)`: Instantiate the NMF model with a specified number of topics.\n",
    "   - `fit_transform(tfidf_matrix)`: Fit the model to the TF-IDF matrix and return the topic matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF topic matrix shape: (11314, 100)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the NMF topic matrix\n",
    "print(f\"NMF topic matrix shape: {nmf_topics.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate CountVectorizer with specified parameters\n",
    "count_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **CountVectorizer**:\n",
    "   - `max_df=0.95`: Ignore terms that appear in more than 95% of the documents.\n",
    "   - `min_df=2`: Ignore terms that appear in fewer than 2 documents.\n",
    "   - `max_features=no_features`: Limit the number of features (terms) to `no_features`.\n",
    "   - `stop_words='english'`: Remove common English stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the fit_transform method to transform the documents\n",
    "count_matrix = count_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Transform the Documents**:\n",
    "   - `fit_transform(documents)`: Learn the vocabulary and return the term-document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names from CountVectorizer\n",
    "count_feature_names = count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Get Feature Names**:\n",
    "   - `get_feature_names_out()`: Retrieve the feature names (terms) learned by the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 100\n",
      "First 10 feature names: ['00' '10' '12' '14' '15' '16' '20' '25' 'a86' 'available']\n"
     ]
    }
   ],
   "source": [
    "# Print the number of features and the first 10 feature names\n",
    "print(f\"Number of features: {len(count_feature_names)}\")\n",
    "print(f\"First 10 feature names: {count_feature_names[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate LatentDirichletAllocation and fit_transform the count data\n",
    "num_topics = 100\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=1)\n",
    "lda_topics = lda_model.fit_transform(count_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **LatentDirichletAllocation (LDA)**:\n",
    "   - `LatentDirichletAllocation(n_components=num_topics, random_state=1)`: Instantiate the LDA model with a specified number of topics.\n",
    "   - `fit_transform(count_matrix)`: Fit the model to the term-document matrix and return the topic matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA topic matrix shape: (11314, 100)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the LDA topic matrix\n",
    "print(f\"LDA topic matrix shape: {lda_topics.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Top Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display topics for NMF model\n",
    "def display_nmf_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "people know mr 14 different 25 set read let ll\n",
      "Topic 1:\n",
      "does know 14 set different read available 25 question didn\n",
      "Topic 2:\n",
      "know does read question didn years god don drive edu\n",
      "Topic 3:\n",
      "edu 14 file mr set different read available 25 let\n",
      "Topic 4:\n",
      "just a86 things don years going doesn drive edu fact\n",
      "Topic 5:\n",
      "like mr read different 25 file available set ll question\n",
      "Topic 6:\n",
      "just years good doesn don drive edu fact far file\n",
      "Topic 7:\n",
      "use max set don read 10 question years god drive\n",
      "Topic 8:\n",
      "thanks max set read file does question good doesn don\n",
      "Topic 9:\n",
      "good mr different read let available ll key didn question\n",
      "Topic 10:\n",
      "think don set read question didn case drive edu fact\n",
      "Topic 11:\n",
      "god things don mr jesus believe ll new let question\n",
      "Topic 12:\n",
      "problem 14 file question didn years going drive edu fact\n",
      "Topic 13:\n",
      "windows read set different 25 key question g9v doesn don\n",
      "Topic 14:\n",
      "drive max different mr set read 25 file key question\n",
      "Topic 15:\n",
      "time max ll 10 let question didn know key drive\n",
      "Topic 16:\n",
      "don different ll case question going drive edu fact far\n",
      "Topic 17:\n",
      "ve question didn going doesn don drive edu fact far\n",
      "Topic 18:\n",
      "bit max 10 file let available years good drive edu\n",
      "Topic 19:\n",
      "com max mr 25 file read available different let set\n",
      "Topic 20:\n",
      "need max 10 file ll read set available years going\n",
      "Topic 21:\n",
      "used available file question years good don drive edu fact\n",
      "Topic 22:\n",
      "year 14 10 ll new available didn question good fact\n",
      "Topic 23:\n",
      "right 10 question years going don drive edu fact far\n",
      "Topic 24:\n",
      "key make 14 want space let government question didn 25\n",
      "Topic 25:\n",
      "make 14 ll let question didn case going drive edu\n",
      "Topic 26:\n",
      "mail file available let ll years good don drive edu\n",
      "Topic 27:\n",
      "way ll let years going doesn don drive edu fact\n",
      "Topic 28:\n",
      "things say mr going let case didn 15 doesn drive\n",
      "Topic 29:\n",
      "10 b8f a86 question years good drive edu fact far\n",
      "Topic 30:\n",
      "want max mr different make 25 ll let read key\n",
      "Topic 31:\n",
      "really mr read 10 didn years going drive edu fact\n",
      "Topic 32:\n",
      "said mr 25 let file ll 10 set going didn\n",
      "Topic 33:\n",
      "say just ll different let going question didn case don\n",
      "Topic 34:\n",
      "space max 14 file different ll 10 available let read\n",
      "Topic 35:\n",
      "did mr just set 25 different let going got world\n",
      "Topic 36:\n",
      "work max just 14 different file ll 10 available let\n",
      "Topic 37:\n",
      "file a86 g9v max 25 set read world key program\n",
      "Topic 38:\n",
      "believe just file 10 going got question didn drive edu\n",
      "Topic 39:\n",
      "got 14 file ll let going didn don drive edu\n",
      "Topic 40:\n",
      "using 14 file 10 ll available years got drive edu\n",
      "Topic 41:\n",
      "sure just ll make file going let world god doesn\n",
      "Topic 42:\n",
      "question just ll 10 going didn years drive edu fact\n",
      "Topic 43:\n",
      "course little years mr use better want 25 going let\n",
      "Topic 44:\n",
      "going just ll let got didn key g9v doesn don\n",
      "Topic 45:\n",
      "ll b8f g9v just going got good don drive edu\n",
      "Topic 46:\n",
      "probably just ll let different going don drive edu fact\n",
      "Topic 47:\n",
      "years max 10 ll let just new world going got\n",
      "Topic 48:\n",
      "program 14 know file let available ll make 10 going\n",
      "Topic 49:\n",
      "help know mr file 25 years going don drive edu\n",
      "Topic 50:\n",
      "true max set just read file let world going case\n",
      "Topic 51:\n",
      "know problem help 14 just make let ll new got\n",
      "Topic 52:\n",
      "better max a86 just ll best going got god g9v\n",
      "Topic 53:\n",
      "case just ll years going don drive edu fact far\n",
      "Topic 54:\n",
      "software 14 set does available different file 10 let key\n",
      "Topic 55:\n",
      "information max 14 file mr available 10 read key world\n",
      "Topic 56:\n",
      "best 10 world going didn good don drive edu fact\n",
      "Topic 57:\n",
      "number 14 file available 10 let years drive edu fact\n",
      "Topic 58:\n",
      "read ax just file ll let going years don drive\n",
      "Topic 59:\n",
      "doesn just mr different read ll key set let going\n",
      "Topic 60:\n",
      "didn just read ll different let file key world going\n",
      "Topic 61:\n",
      "power max 14 set 10 just let new ll far\n",
      "Topic 62:\n",
      "list 14 file 10 different make available let ll read\n",
      "Topic 63:\n",
      "data ax 14 just file 10 available let government going\n",
      "Topic 64:\n",
      "tell just ll 10 set let world going far government\n",
      "Topic 65:\n",
      "let a86 g9v just ll read far going government best\n",
      "Topic 66:\n",
      "world years 14 little better long time use power just\n",
      "Topic 67:\n",
      "government people little point years just mr time state use\n",
      "Topic 68:\n",
      "lot just people different read ll let going got far\n",
      "Topic 69:\n",
      "00 14 set make good don drive edu fact far\n",
      "Topic 70:\n",
      "look mr just file like ll let best far got\n",
      "Topic 71:\n",
      "try max just file 10 let ll going got don\n",
      "Topic 72:\n",
      "long max just 10 far ll available best got going\n",
      "Topic 73:\n",
      "law 14 people help just file read fact 10 available\n",
      "Topic 74:\n",
      "available a86 g9v different key far best government program tell\n",
      "Topic 75:\n",
      "set ax 14 file just available let far best tell\n",
      "Topic 76:\n",
      "day mr just read jesus different 10 ll key available\n",
      "Topic 77:\n",
      "thing max just people read far going key best got\n",
      "Topic 78:\n",
      "line file just 10 let ll far available best years\n",
      "Topic 79:\n",
      "run max just file 10 ll let available far got\n",
      "Topic 80:\n",
      "little max just mr ll different 10 let read world\n",
      "Topic 81:\n",
      "state max people mr just 14 read file fact ll\n",
      "Topic 82:\n",
      "second 14 people just 10 let fact far going jesus\n",
      "Topic 83:\n",
      "fact just people 14 little years time point true better\n",
      "Topic 84:\n",
      "different ax just time little use like far jesus best\n",
      "Topic 85:\n",
      "come 14 people mr just jesus 25 read like let\n",
      "Topic 86:\n",
      "far people just mr let jesus 25 key going best\n",
      "Topic 87:\n",
      "jesus say god point 14 16 does just people little\n",
      "Topic 88:\n",
      "12 14 10 25 just ll 20 let available new\n",
      "Topic 89:\n",
      "mr ax b8f g9v a86 16 people just 00 fact\n",
      "Topic 90:\n",
      "20 14 god 10 day fact best got years edu\n",
      "Topic 91:\n",
      "15 max 14 say 10 file god ll 25 let\n",
      "Topic 92:\n",
      "16 max 14 10 god ll file available let new\n",
      "Topic 93:\n",
      "25 ax b8f 16 g9v 14 15 just people god\n",
      "Topic 94:\n",
      "14 a86 b8f g9v max people help does god 20\n",
      "Topic 95:\n",
      "point max say people just different does like fact think\n",
      "Topic 96:\n",
      "max ax b8f a86 g9v know 00 like ve said\n",
      "Topic 97:\n",
      "key say 16 use little time point bit just information\n",
      "Topic 98:\n",
      "bit course things say make does help people like believe\n",
      "Topic 99:\n",
      "new b8f max say people does god like fact think\n"
     ]
    }
   ],
   "source": [
    "# Display the top words in each topic for the NMF model\n",
    "no_top_words = 10  # Number of top words to display for each topic\n",
    "display_nmf_topics(nmf_model, tfidf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Display Topics Function**:\n",
    "   - The `display_nmf_topics` function takes the NMF model, feature names, and the number of top words to display for each topic. It prints out the top words for each topic by sorting the words based on their importance in the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display topics\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "got just come way like make good don right didn\n",
      "Topic 1:\n",
      "say just don way like come let fact course make\n",
      "Topic 2:\n",
      "said people didn know don like say just time did\n",
      "Topic 3:\n",
      "best way don come just make course good does know\n",
      "Topic 4:\n",
      "list way know don new use point ll need like\n",
      "Topic 5:\n",
      "run just like way don make come course time using\n",
      "Topic 6:\n",
      "ve just like way don know got time years think\n",
      "Topic 7:\n",
      "list long like just good come believe 15 number way\n",
      "Topic 8:\n",
      "problem using time just don try way make does did\n",
      "Topic 9:\n",
      "point just like course good make don come fact know\n",
      "Topic 10:\n",
      "00 20 15 new 10 25 world software list edu\n",
      "Topic 11:\n",
      "probably way don just like make come know little really\n",
      "Topic 12:\n",
      "things way like come don believe know just people doesn\n",
      "Topic 13:\n",
      "using use way used does make work information like available\n",
      "Topic 14:\n",
      "said just like people way don come time believe say\n",
      "Topic 15:\n",
      "sure just like don way make know believe come good\n",
      "Topic 16:\n",
      "use way don just like using know need does make\n",
      "Topic 17:\n",
      "going just way don make like know good right years\n",
      "Topic 18:\n",
      "government think don make fact just way let people does\n",
      "Topic 19:\n",
      "far course way know like just come time don people\n",
      "Topic 20:\n",
      "help way just come know need like does using make\n",
      "Topic 21:\n",
      "work think people going lot ve want time program try\n",
      "Topic 22:\n",
      "new like come just 20 world years time make 15\n",
      "Topic 23:\n",
      "25 way new ll world make list help people com\n",
      "Topic 24:\n",
      "question course way don know come does true like make\n",
      "Topic 25:\n",
      "program long max number use time set information does way\n",
      "Topic 26:\n",
      "key people make good say using want don used doesn\n",
      "Topic 27:\n",
      "line just way like don using try look does need\n",
      "Topic 28:\n",
      "long world way time just come years fact like make\n",
      "Topic 29:\n",
      "lot don like just come way know make things believe\n",
      "Topic 30:\n",
      "mail like make world come new don edu just know\n",
      "Topic 31:\n",
      "12 14 10 15 come new 25 time just like\n",
      "Topic 32:\n",
      "thanks mail need just know does let don jesus law\n",
      "Topic 33:\n",
      "read don just does people time good use make know\n",
      "Topic 34:\n",
      "edu com just like list way world state max don\n",
      "Topic 35:\n",
      "file use set case new used information like 15 right\n",
      "Topic 36:\n",
      "information new like does people number available know used use\n",
      "Topic 37:\n",
      "case fact don just way course know think make like\n",
      "Topic 38:\n",
      "windows using just like use don way come know program\n",
      "Topic 39:\n",
      "time like just don come make years people good think\n",
      "Topic 40:\n",
      "just way make course think law does like time different\n",
      "Topic 41:\n",
      "second just way come make time like new know good\n",
      "Topic 42:\n",
      "know don just let way like time course say read\n",
      "Topic 43:\n",
      "really don just like know way make people think come\n",
      "Topic 44:\n",
      "didn know just way did don make doesn like time\n",
      "Topic 45:\n",
      "work just way like don try make using know doesn\n",
      "Topic 46:\n",
      "need like just does make want new using time work\n",
      "Topic 47:\n",
      "ax max g9v b8f a86 mr 14 25 ll 16\n",
      "Topic 48:\n",
      "people does just course believe thing things say question way\n",
      "Topic 49:\n",
      "little like just new right don time want know try\n",
      "Topic 50:\n",
      "com don just like way try use make need want\n",
      "Topic 51:\n",
      "day just way come little like people don good course\n",
      "Topic 52:\n",
      "don know think just believe try like people long way\n",
      "Topic 53:\n",
      "make don like way know just say want people believe\n",
      "Topic 54:\n",
      "need don know doesn right like going years 20 used\n",
      "Topic 55:\n",
      "data software using like use way time used new point\n",
      "Topic 56:\n",
      "jesus god people say come does fact really way believe\n",
      "Topic 57:\n",
      "tell just like know course people doesn don does make\n",
      "Topic 58:\n",
      "believe way point course true fact used different say does\n",
      "Topic 59:\n",
      "thanks know like does way just make using use believe\n",
      "Topic 60:\n",
      "drive way just know like use max try don make\n",
      "Topic 61:\n",
      "mr don know think day did sure ll said time\n",
      "Topic 62:\n",
      "key use using way used like people fact just don\n",
      "Topic 63:\n",
      "believe does know use using doesn like program time come\n",
      "Topic 64:\n",
      "good don just like way believe know make course come\n",
      "Topic 65:\n",
      "right people just way come make like does believe know\n",
      "Topic 66:\n",
      "years fact make number use way like new time 15\n",
      "Topic 67:\n",
      "10 15 20 make course just like way let know\n",
      "Topic 68:\n",
      "16 drive data use way does run right using second\n",
      "Topic 69:\n",
      "god believe world people say fact does come know way\n",
      "Topic 70:\n",
      "did like fact just people believe make don know course\n",
      "Topic 71:\n",
      "does know like just come way don make using course\n",
      "Topic 72:\n",
      "think don just way like come know good make believe\n",
      "Topic 73:\n",
      "true try way come like using make use think things\n",
      "Topic 74:\n",
      "bit 16 just max like using way use does time\n",
      "Topic 75:\n",
      "let ll come didn like time better just years know\n",
      "Topic 76:\n",
      "set course way like using just use know used don\n",
      "Topic 77:\n",
      "look just like way don come know doesn want people\n",
      "Topic 78:\n",
      "number don used just using 15 time like people use\n",
      "Topic 79:\n",
      "government fact new use come law using used does little\n",
      "Topic 80:\n",
      "20 15 16 10 14 12 25 new way max\n",
      "Topic 81:\n",
      "thing just course way like don know make people come\n",
      "Topic 82:\n",
      "want just don come people way believe know like make\n",
      "Topic 83:\n",
      "used way like just don know good years long time\n",
      "Topic 84:\n",
      "use used run like different new point need does problem\n",
      "Topic 85:\n",
      "different way just don does data know like time right\n",
      "Topic 86:\n",
      "come people don think time just things want say good\n",
      "Topic 87:\n",
      "power like just using don way new 20 use make\n",
      "Topic 88:\n",
      "year years way don just like make good think 20\n",
      "Topic 89:\n",
      "law fact believe use come way right does point just\n",
      "Topic 90:\n",
      "software use good does want time don better work look\n",
      "Topic 91:\n",
      "doesn just problem like know don way try come make\n",
      "Topic 92:\n",
      "state years people just way fact world time come new\n",
      "Topic 93:\n",
      "ll just like way course make know say years new\n",
      "Topic 94:\n",
      "ll going ve don know believe work does number think\n",
      "Topic 95:\n",
      "space program data 15 long new use time information available\n",
      "Topic 96:\n",
      "better just way don like come course think make want\n",
      "Topic 97:\n",
      "like just don come know let way course people case\n",
      "Topic 98:\n",
      "available software use program set edu information number mail data\n",
      "Topic 99:\n",
      "people world just fact way like make think know course\n"
     ]
    }
   ],
   "source": [
    "# Display the top words in each topic for the LDA model\n",
    "no_top_words = 10  # Number of top words to display for each topic\n",
    "display_topics(lda_model, count_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Display Topics Function**:\n",
    "   - The `display_topics` function takes a model, feature names, and the number of top words to display for each topic. It prints out the top words for each topic by sorting the words based on their importance in the topic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
